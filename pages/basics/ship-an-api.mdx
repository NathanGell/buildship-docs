# How to SHIP an API

import { Callout } from 'nextra/components';
import Image from 'next/image';
import ship1 from '/public/basics/ship-1.png';
import ship2 from '/public/basics/ship-2.png';
import ship3 from '/public/basics/ship-3.png';
import ship4 from '/public/basics/ship-4.png';
import ship5 from '/public/basics/ship-5.png';
import ship6 from '/public/basics/ship-6.png';
import ship7 from '/public/basics/ship-7.png';
import ship8 from '/public/basics/ship-8.png';
import ship9 from '/public/basics/ship-9.png';
import ship10 from '/public/basics/ship-10.png';
import ship11 from '/public/basics/ship-11.png';
import ship12 from '/public/basics/ship-12.png';
import ship13 from '/public/basics/ship-13.png';
import ship14 from '/public/basics/ship-14.png';
import ship15 from '/public/basics/ship-15.png';
import ship16 from '/public/basics/ship-16.png';

Building APIs is a complex process especially when you need to integrate multiple 3rd party integrations. BuildShip
empowers you to effortlessly design, deploy, and manage APIs, providing a user-friendly platform that accelerates your
development cycle.

Learn how to build APIs step-by-step, leverage AI to build logic, focus on your business logic rather than intricate
technical details.

## How to create an API?

To begin creating an API, the initial step is to break down the API into smaller chunks. For example, let‚Äôs say we wish
**to build an app which generates custom audio story books based on a prompt** provided by the end user of your app and
also provide a cover image.

What we understand till now is we will call an API from our app‚Äôs front end which is gonna fetch the audio and cover
image for the prompt provided by the user and display it on the front end.

But how do we break this down into small achievable steps? Let‚Äôs build this app together and see...

### Step 1: When will the API be called?

According to our example, we are supposed to have an input on the front end for the user to enter the prompt for the
audio story book. The user would then hit a ‚ÄúGenerate‚Äù button which should send the prompt to the API, generate the
audio story book and return it to the front end.

The ‚Äú**Generate**‚Äù button sends an HTTP request to an API Endpoint which we‚Äôll create using BuildShip. This is the
starting point of our API, the HTTP Trigger.

<Image src={ship1} alt='ship1' width={1200} />

### Step 2: Specify the Request Structure

Now let us define the HTTP Request structure that is going to trigger the API Endpoint. An API Request has the following
structure.

<Image src={ship2} alt='ship2' width={1200} />

The **Request Endpoint** will be the Rest API Call Endpoint that we set, which is
`https://008vd0.buildship.run/story-gen`.

For the **Request Method**, we can pick from `GET`, `POST`, `PUT`, and `DELETE`. Each method has it‚Äôs own actions and
characteristics. To learn more about each of these methods, check out the
[MDN Docs website](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods). For this example, we‚Äôll use the `POST`
request method.

The **Query Params** are the additional parameters to be appended to the end of the endpoint URL. We do have an option
to pass in the prompt via the Query Params as shown in the image above, but for this example we‚Äôll pass in the `prompt`
with the `style` value in the request body.

We can also optionally add the **HTTP Headers** to pass additional information about the HTTP Request. To learn more
about the Headers to include, refer to the
[MDN Docs website](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers).

<Callout>
  **TIPüí°:** We also have a documentation on a very interesting use-case utilizing the Authorization Request Header to
  implement ‚ÄúBring Your Own Key‚Äù (BYOK) using BuildShip. [Learn more](https://docs.buildship.com/tutorials/byok).
</Callout>

For the request body, let‚Äôs pass in the values we require for our API from the user which are:

- **Prompt** - The main story line for the generation
- **Style** - The author style for the story being generated. Let‚Äôs say you wish to generate a kids fairytale so we can
  input ‚ÄúBrothers Grimm‚Äù to generate similar stories.

The request body should look something like this:

```
{
  "prompt": "Two siblings discover the hidden mysteries of an enchanted garden",
  "style": "Brothers Grimm"
}
```

<Callout>
For making it easy to [access the values](https://docs.buildship.com/basics/api-spec#accessing-request-values) for our request structure in the workflow input fields, we can specify the Request Structure in the REST API Call Trigger. [Learn more](https://docs.buildship.com/basics/api-spec#request-specification).

<Image src={ship3} alt="ship3" width={1200} />
</Callout>

### Step 3: Generate the Story

We can now get the `prompt` and `style` values from the request body, which we can now directly access from the context
menu.

Now we need to generate our story via the prompt given by the user. Let‚Äôs use the OpenAI GPT Text Generator node to
generate the story, along with the cover image description, for us.

Add the OpenAI API Key (which we can get from the
[OpenAI Developer Dashboard](https://platform.openai.com/account/api-keys)).

<Callout>
  We can securely store our API Keys in the BuildShip Secret Manager to maintain confidentiality and avoid misuse.
  [Learn more](https://docs.buildship.com/secrets).
</Callout>

For the **System Prompt**, let‚Äôs add:

```jsx
`You're a ghost writer for ${ctx['root']['request']['body']['style'] ?? 'Beatrix Potter'},
     users give you prompts and you write short children story, in pages format,with short discription of an illustration to support the text,without mentioning the character names in the illustration text.
      Respond in json format{
      "cover":{
      "title":string,
      "subtitle":string,
      "illustration":string
      },
      "pages":[
      {index:number,
      "illustration":string,
      "text":string
      }
      ]
      }
      You are bedtime story teller. You will create:
      - A short bedtime stories that are meaningful, have a good moral, and creative
      - It should be safe for children to hear and no explicit content
      - No more than 6 pages
      - If you are unsure how to create a story say "{"error":"Sorry, I can't tell a story on that topic"}.
      `;
```

For the **User Prompt**, let‚Äôs pass the incoming prompt from the HTTP Request Body.

<Image src={ship4} alt='ship4' width={1200} />

<Callout>
At any point, while building your BuildShip Workflow, we can use the Workflow and Node testing to check outputs and confirm the workflow is working correctly.

<Image src={ship5} alt="ship5" width={1200} />
</Callout>

### Step 4: Parsing the JSON output

Now we see that we get a stringified JSON object. Now to convert this into a JSON object we‚Äôll be using the **Parse
JSON** node to do this.

<Image src={ship6} alt='ship6' width={1200} />

We can also test and ensure everything is working smoothly till this step.

### Step 5: Running flows in Parallel

Now we need to:

1. Generate the audio file for the story
2. Generate the cover image for the storybook

Both of these processes can be run in parallel. Let‚Äôs add a Parallel Execution node. Now, let‚Äôs discuss each of these
processes one by one.

<Image src={ship7} alt='ship7' width={1200} />

### PARALLEL PROCESS 1: Generating a Cover Image

**GENERATE IMAGE**

Firstly, we need a node to generate an image based on the prompt provided by the **GPT Text Generator Node.**

<Callout>
If you search ‚ÄúGenerate Image‚Äù in the node explorer, you‚Äôll see BuildShip comes with a bunch of pre-built integration nodes for the same purpose - OpenAI DALL-E, StabilityAI Image Generator, Hugging Face Text to Image, Replicate Image Generation.

You can use any of these pre-built nodes for generating the cover image.

</Callout>

For this example, we‚Äôll be using the **OpenAI Image Generator** node using the DALL-E Model. Add in your API Key and
pick the DALL-E Model. For the Input Text, we‚Äôll be passing in the value for the `cover.illustration` from our Parsed
OpenAI JSON.

<Image src={ship8} alt='ship8' width={1200} />

Pick a size. For this example, we‚Äôll be using 1024 X 1024.

Clicking on the ‚ÑπÔ∏è¬†icon on the Image Generator node reveals a node info section which tells us everything we need to
know about that node. This reveals that the generated output will be a Base64 file.

<Image src={ship9} alt='ship9' width={1200} />

**SAVE BASE64 FILE**

Let‚Äôs search the nodes featuring the Base64 format. We see we have two nodes:

- Upload Base64 file to **BuildShip File Storage**
- Upload Base64 file to **Google Cloud Storage**

For this example, let‚Äôs use the BuildShip File Storage node which requires no set-up. We‚Äôll be able to upload the Base64
file to a folder in the File Storage and returns the generated Public URL.

<Callout>
We can also access the files via the BuildShip Storage Manager available in the Project Settings.

<Image src={ship10} alt="ship10" width={1200} />
</Callout>

For the \***\*Upload Base64 File - BuildShip Storage\*\*** node, add the output file from the **OpenAI Image Generator**
node.

For the file name, we‚Äôll need to add a unique name every time a story is generated. We can achieve this by adding a UUID
Generator node before the **Parallel Execution**, after the **Parse JSON** node.

In the File Name input for the Upload Base64 File node, we can define the entire path for storing the Image file. For
example:

<Image src={ship11} alt='ship11' width={1200} />

Our flow to generate the cover image for the story. Now, let‚Äôs build the parallel flow for generating the audio file for
the Storybook.

### PARALLEL PROCESS 2: Generating the Audio Storybook

For generating the audio file for each of the story book page, we‚Äôll require a loop. This will loop over each page‚Äôs
text, generate an audio file, and upload the file to the BuildShip File Storage in each iteration.

**ADDING A LOOP**

The Items input requires an array to loop through. Let‚Äôs add the `pages` array from the parsed JSON input provided by
OpenAI (from the **Parse JSON** node).

<Image src={ship12} alt='ship12' width={1200} />

**CONVERT TEXT TO SPEECH**

Now, within the loop, we need to add the OpenAI Text to Speech node which will convert the story pages into audio page
by page with every iteration.

Add in the API Key and pick a model. The Input Text is the text for the current iteration page, i.e. `item.text` .

<Image src={ship13} alt='ship13' width={1200} />

**SAVING THE AUDIO FILE TO STORAGE**

Once one iteration of the loop generates the audio it will output a Base64 file which we need to save to the BuildShip
storage. So, the audio file generated in every iteration is collected in a folder to be combined later.

The Base64 File will be the output generated by the Text to Speech node for that iteration. Let‚Äôs set the File Path to:

<Image src={ship14} alt='ship14' width={1200} />

### Step 6: Combining the Audio Files

All we need to do now is to combine all the audio files together. For this, we do not already have a pre-built node in
the library, so let‚Äôs create it using the ‚Äú**Generate with AI**‚Äù feature.

Take the following prompt for example:

```
I need a node that given a list of public urls of a base64 audio file, it combines them together into one file and returns it's public url. You can use the audioconcat library which uses ffmpeg.
```

For the **Audio URLs** add in the output from the **Loop** node, as shown in the Image below.

<Image src={ship15} alt='ship15' width={1200} />

Now, we can test the workflow and see if everything is working as expected and return the output as shown below:

<Image src={ship16} alt='ship16' width={1200} />
