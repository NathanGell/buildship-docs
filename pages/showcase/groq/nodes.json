[
  {
    "subType": "assistant",
    "_createdBy": {
      "displayName": "Save to Library",
      "timestamp": { "_seconds": 1715877737, "_nanoseconds": 618000000 }
    },
    "type": "script",
    "integrations": [],
    "live": true,
    "category": ["Integrations"],
    "featured": false,
    "output": {
      "buildship": { "index": 0 },
      "description": "",
      "title": "",
      "type": "object",
      "properties": {
        "threadId": { "buildship": { "index": 1 }, "description": "", "title": "Thread Id", "type": "string" },
        "data": { "buildship": { "index": 2 }, "description": "", "type": "string", "title": "Data" },
        "message": { "buildship": { "index": 0 }, "description": "", "type": "string", "title": "Message" }
      }
    },
    "inputs": {
      "type": "object",
      "required": ["groqApiKey", "systemPrompt", "userPrompt", "model", "maxTokens"],
      "properties": {
        "threadId": {
          "default": "",
          "buildship": { "index": 5, "sensitive": false },
          "pattern": "",
          "description": "Add the conversation ThreadID to maintain chat history",
          "title": "Thread Id",
          "type": "string"
        },
        "systemPrompt": {
          "default": "",
          "buildship": { "index": 1, "sensitive": false },
          "pattern": "",
          "description": "System prompt for use in your Groq AI Assistant. Example:\n`You are a poet`",
          "title": "Instructions",
          "type": "string"
        },
        "userPrompt": {
          "default": "",
          "buildship": { "index": 2, "sensitive": false },
          "pattern": "",
          "description": "Pass the user's message to the AI Assistant from the incoming trigger connector or prior nodes",
          "title": "User Prompt",
          "type": "string"
        },
        "maxTokens": {
          "default": 1024,
          "buildship": { "index": 4, "sensitive": false },
          "pattern": "",
          "description": "Define the maximum tokens you want to use in the AI Assistant per call. Defaults to `1024`",
          "title": "Max Tokens",
          "type": "number"
        },
        "model": {
          "default": "llama3-8b-8192",
          "buildship": {
            "options": [
              { "label": "LLaMA3 70b", "value": "llama3-70b-8192" },
              { "label": "LLaMA3 8b", "value": "llama3-8b-8192" },
              { "label": "Mixtral 8x7b", "value": "mixtral-8x7b-32768" },
              { "label": "Gemma 7b", "value": "gemma-7b-it" }
            ],
            "index": 4,
            "sensitive": false
          },
          "pattern": "",
          "description": "Select the model for use with your Groq AI Assistant. Defaults to `LLaMA3 8b`",
          "title": "Model",
          "type": "string",
          "enum": ["llama3-70b-8192", "llama3-8b-8192", "mixtral-8x7b-32768", "gemma-7b-it"]
        },
        "groqApiKey": {
          "buildship": { "index": 0, "sensitive": true },
          "pattern": "",
          "description": "Add Groq account's [API Key](https://console.groq.com/keys)",
          "title": "API Key",
          "type": "string"
        }
      }
    },
    "meta": {
      "name": "Groq AI Assistant",
      "icon": {
        "type": "URL",
        "url": "https://firebasestorage.googleapis.com/v0/b/website-a1s39m.appspot.com/o/buildship-app-logos%2Fgroq.png?alt=media&token=7f60edf7-f402-4762-8334-00cb2981cef0"
      },
      "description": "Groq AI Assistant node for building chatbot and AI workflows that can run fast. Connect to your datasources and perform function calling.",
      "id": "groq-assistant-node"
    },
    "dependencies": {
      "jsonc": "2.0.0",
      "path": "0.12.7",
      "lodash": "4.17.21",
      "fs": "0.0.2",
      "groq-sdk": "0.3.3",
      "uuid": "9.0.1"
    },
    "deployToBucket": {
      "ranBy": "harini@rowy.io.rowy",
      "completedAt": { "_seconds": 1715879095, "_nanoseconds": 446000000 }
    },
    "group": [
      {
        "uid": "groq",
        "name": "Groq",
        "description": "Nodes for leveraging Groq's super-fast LPU Inference Engine.",
        "id": "0IAjU2tekQHjibkvicop",
        "iconUrl": "https://firebasestorage.googleapis.com/v0/b/website-a1s39m.appspot.com/o/buildship-app-logos%2Fgroq.png?alt=media&token=7f60edf7-f402-4762-8334-00cb2981cef0"
      }
    ],
    "groupInfo": "0IAjU2tekQHjibkvicop",
    "syncIndex": {
      "ranBy": "harini@rowy.io.rowy",
      "status": "synced",
      "completedAt": { "_seconds": 1716175437, "_nanoseconds": 289000000 }
    },
    "_updatedBy": {
      "emailVerified": true,
      "photoURL": "https://lh3.googleusercontent.com/a/AAcHTtfPe0yH9QzE2iqPg4aVB_xklqimDxaI5WOsRM9XqSvD=s96-c",
      "uid": "lGcdiftmK4QNZ2XuMUVqrm6B21c2",
      "displayName": "Aaditya Bhusal",
      "email": "aaditya@rowy.io.rowy",
      "timestamp": { "_seconds": 1716278093, "_nanoseconds": 170000000 }
    },
    "integrity": "v3:333d73fbb668b65f76fa207d362bb5c8",
    "src": "https://storage.googleapis.com/buildship-app-us-central1/publicLib/nodes/@buildship/groq-assistant-node/1.1.0/build.cjs",
    "id": "groq-assistant-node",
    "version": "1.1.0",
    "_libRef": {
      "integrity": "v3:afc45210fcfe529c3eec07eff0696b1e",
      "libNodeRefId": "@buildship/groq-assistant-node",
      "isDirty": false,
      "libType": "public",
      "src": "https://storage.googleapis.com/buildship-app-us-central1/publicLib/nodes/@buildship/groq-assistant-node/1.0.1/build.cjs",
      "version": "1.0.1"
    },
    "script": "import Groq from 'groq-sdk';\nimport { snakeCase } from \"lodash\";\nimport fs from \"fs\";\nimport path from \"path\";\nimport { v4 as uuidv4 } from 'uuid';\nimport { jsonc } from 'jsonc';\n\nconst getChatHistory = (threadId: string, logging: any) => {\n  // Load previous messages if the file exists\n  let previousMessages = [];\n  const filePath = process.env.BUCKET_FOLDER_PATH + '/nodes/groq-assistant/store/' + threadId + '.jsonl';\n  if (threadId) {\n    const fileExists = fs.existsSync(filePath);\n    if (fileExists) {\n      const fileContent = fs.readFileSync(filePath, 'utf8');\n      previousMessages = JSON.parse(fileContent);\n      logging.log(previousMessages);\n    }\n  }\n  return previousMessages;\n}\n\nconst appendChatHistory = (threadId: string, newMessages: unknown[]) => {\n  const filePath = process.env.BUCKET_FOLDER_PATH + '/nodes/groq-assistant/store/' + threadId + '.jsonl';\n  // Create folder path if it doesn't exist\n  const folderPath = path.dirname(filePath);\n  if (!fs.existsSync(folderPath)) {\n    fs.mkdirSync(folderPath, { recursive: true });\n  }\n  // Save userRequest and output to a JSONL file\n  const fileContent = JSON.stringify(newMessages);\n  fs.writeFileSync(filePath, fileContent);\n}\n\n\ntype Tool = Groq.Chat.CompletionCreateParams.Tool;\ntype FinishReason = \"stop\" | \"length\" | \"tool_calls\" | \"content_filter\";\n\nconst nodeToGroqTool: (node: Node) => Tool = (node) => {\n  return {\n    type: \"function\",\n    function: {\n      name: snakeCase(node.label || node.meta.name),\n      description: node.meta.description ?? \"\",\n      parameters: {\n        type: \"object\",\n        properties: Object.entries(node.inputs.properties)\n          .reduce((properties, [name, value]) => {\n            if (value.buildship && !value.buildship.toBeAutoFilled) return properties;\n            return {\n              ...properties, [name]: {\n                type: value.type,\n                enum: value.enum,\n                description: value.description\n              }\n            }\n          }, {}),\n        required: Object.entries(node.inputs.properties).map(([name, value]) => {\n          if (value.buildship && value.buildship.toBeAutoFilled && node.inputs.required.includes(name)) return name;\n          return false;\n        }).filter(Boolean),\n      }\n    }\n  };\n}\n\nexport default async function assistant(\n  { groqApiKey, model, maxTokens, userPrompt, systemPrompt, threadId }: NodeInputs,\n  { logging, execute, nodes }: NodeScriptOptions) : NodeOutput  {\n  const groq = new Groq({ apiKey: groqApiKey });\n\n  const tools: Tool[] = nodes?.map(nodeToGroqTool) ?? [];\n\n  /** \n  * Retrieve the conversation from the threadId if it exists, otherwise generate a new threadId\n  **/\n  threadId ||= uuidv4();\n  const chatHistory = getChatHistory(threadId, logging) as Groq.Chat.ChatCompletion.Choice.Message[];\n\n  const initialMessages: Groq.Chat.CompletionCreateParams.Message[] = [\n    {\n      \"role\": \"system\",\n      \"content\": systemPrompt\n    },\n    // append the chat history to the initial messages excluding the system messages\n    ...(chatHistory.filter(m => m.role !== \"system\") ?? []),\n    {\n      \"role\": \"user\",\n      \"content\": userPrompt,\n    }\n  ];\n\n  const baseRequest = {\n    \"model\": model,\n    \"max_tokens\": maxTokens,\n    \"tools\": tools,\n    \"messages\": initialMessages\n  };\n\n  try {\n    let requestCount = 1;\n    let request = { ...baseRequest };\n    let response: Groq.Chat.ChatCompletion;\n\n    let finish_reasons: FinishReason[] = [];\n\n    const isEndTurn = (reasons: FinishReason[]) =>\n      reasons.includes(\"stop\") ||\n      reasons.includes(\"length\") ||\n      reasons.includes(\"content_filter\");\n\n    do {\n      logging.log(`Groq request(${requestCount}):`, request);\n      response = await groq.chat.completions.create(request);\n      logging.log(`Groq response(${requestCount}): `, response);\n\n      const choices = response.choices;\n      finish_reasons = choices.map(choice => choice.finish_reason) as FinishReason[];\n\n      if (isEndTurn(finish_reasons)) {\n        break;\n      }\n      for (const choice of choices) {\n        request.messages.push(choice.message);\n\n        const finish_reason = choice.finish_reason as FinishReason;\n        const isToolUse = finish_reason === \"tool_calls\";\n\n        if (isToolUse) {\n          const toolCalls = choice.message.tool_calls || [];\n\n          for (const toolCall of toolCalls) {\n            const node: Node = nodes?.find((node: Node) =>\n              snakeCase(node.label || node.meta.name) === toolCall.function?.name);\n            if (!node) {\n              logging.log(\"Failed to find tool:\");\n              logging.log(toolCall);\n              logging.log(node);\n              throw new Error(\"Failed to find tool\");\n            }\n            logging.log(`Tool: ${node.label} `);\n            let args = {} as Record<string, unknown>;\n            try {\n              args = JSON.parse(toolCall.function?.arguments ?? \"{}\");\n            } catch (cause) {\n              logging.log(\"Failed to parse tool arguments\");\n              logging.log(toolCall.function?.arguments);\n              logging.log(cause);\n            }\n\n            // filter hallucinated inputs\n            const inputs = {} as Record<string, unknown>;\n            for (const [inputKey, inputValue] of Object.entries(args)) {\n              if (node.inputs.properties[inputKey]) {\n                inputs[inputKey] = inputValue;\n              }\n            }\n            const toolResponse = await execute(node.label, inputs);\n            logging.log(\"Tool response: \", toolResponse);\n            request.messages.push(\n              {\n                \"tool_call_id\": toolCall.id,\n                \"role\": \"tool\",\n                \"name\": toolCall.function?.name,\n                \"content\": toolResponse ? JSON.stringify(toolResponse) : \"\",\n              });\n          }\n        }\n      }\n      requestCount++;\n    } while (!isEndTurn(finish_reasons));\n\n    let newChatHistory = [...request.messages, ...(response.choices.map(c => c.message) || [])]\n    appendChatHistory(threadId, newChatHistory);\n    return {\n      message: response.choices[0]?.message?.content || \"No Response\",\n      threadId,\n      data: response\n    }\n  } catch (error) {\n    logging.log(\"Error:\");\n    logging.log(\n      // remove circular references\n      jsonc.parse(jsonc.stringify(error))\n    );\n    return { error }\n  }\n}\n\ntype Node = {\n  label: string;\n  meta: {\n    id: string;\n    description: string;\n    name: string;\n    [key: string]: any;\n  };\n  inputs: {\n    type: string;\n    required: string[];\n    properties: Record<string, {\n      description: string;\n      buildship?: {\n        toBeAutoFilled?: boolean;\n        [key: string]: any;\n      }\n      [key: string]: any;\n    }>;\n  };\n  [key: string]: any;\n};\n\n",
    "failCount": 2,
    "deployedAt": { "_seconds": 1717467670, "_nanoseconds": 52000000 },
    "usageCounter": 129
  },
  {
    "output": { "buildship": {}, "title": "output", "type": "string" },
    "meta": {
      "name": "Groq Chat",
      "icon": {
        "type": "URL",
        "url": "https://firebasestorage.googleapis.com/v0/b/website-a1s39m.appspot.com/o/buildship-app-logos%2Fgroq.png?alt=media&token=7f60edf7-f402-4762-8334-00cb2981cef0"
      },
      "description": "Using the Groq (LLM) Chat Completions API that processes messages and generates output responses.",
      "id": "groq-chat"
    },
    "type": "script",
    "integrations": [],
    "dependencies": { "groq-sdk": "0.3.2" },
    "category": ["Integrations"],
    "group": [
      {
        "uid": "groq",
        "name": "Groq",
        "description": "Nodes for leveraging Groq's super-fast LPU Inference Engine.",
        "id": "0IAjU2tekQHjibkvicop",
        "iconUrl": "https://firebasestorage.googleapis.com/v0/b/website-a1s39m.appspot.com/o/buildship-app-logos%2Fgroq.png?alt=media&token=7f60edf7-f402-4762-8334-00cb2981cef0"
      }
    ],
    "live": true,
    "groupInfo": "0IAjU2tekQHjibkvicop",
    "deployToBucket": {
      "ranBy": "harini@rowy.io.rowy",
      "completedAt": { "_seconds": 1713756625, "_nanoseconds": 891000000 }
    },
    "inputs": {
      "type": "object",
      "required": ["apiKey", "systemPrompt", "userPrompt", "model"],
      "properties": {
        "systemPrompt": {
          "default": "",
          "buildship": { "index": 1, "sensitive": false },
          "pattern": "",
          "description": "The System Prompt for the Groq Chat Completion.\n\n**SAMPLE INPUT:**\n```\nYou're a helpful assistant.\n```",
          "title": "System Prompt",
          "type": "string"
        },
        "temp": {
          "default": 0.5,
          "buildship": { "index": 4, "sensitive": false },
          "pattern": "",
          "description": "The temperature decides the randomness of the response from the model. Range: `0-1`. Default: `0.5`.",
          "title": "Temperature",
          "type": "number"
        },
        "apiKey": {
          "buildship": { "index": 0, "sensitive": true },
          "pattern": "",
          "description": "The Groq API Key. Get yours from the [GroqCloud API Dashboard](https://console.groq.com/keys).",
          "title": "API Key",
          "type": "string"
        },
        "userPrompt": {
          "default": "",
          "buildship": { "index": 2, "sensitive": false },
          "pattern": "",
          "description": "The User Prompt for the Groq Chat Completion.\n\n**SAMPLE INPUT:**\n```\nExplain the importance of low latency LLMs\n```",
          "title": "User Prompt",
          "type": "string"
        },
        "model": {
          "default": "",
          "buildship": {
            "options": [
              { "label": "LLaMA3 70b", "value": "llama3-70b-8192" },
              { "label": "LLaMA3 8b", "value": "llama3-8b-8192" },
              { "label": "LLaMA2 70b", "value": "llama2-70b-4096" },
              { "label": "Mixtral 8x7b", "value": "mixtral-8x7b-32768" },
              { "label": "Gemma 7b", "value": "gemma-7b-it" }
            ],
            "index": 3,
            "sensitive": false
          },
          "pattern": "",
          "description": "The model to use for chat completion.",
          "type": "string",
          "title": "Model",
          "enum": ["llama3-70b-8192", "llama3-8b-8192", "llama2-70b-4096", "mixtral-8x7b-32768", "gemma-7b-it"]
        }
      }
    },
    "_updatedBy": {
      "uid": "Nb5Sn267f6eOTgqnlB1d6Hxi5VR2",
      "emailVerified": true,
      "photoURL": "https://lh3.googleusercontent.com/a-/AFdZucpItz8EJNSW_ttNzT-N4cG9IQVT5Ipuvbq7xx5Z=s96-c",
      "updatedField": "syncIndex",
      "displayName": "Bhavya Verma",
      "email": "bhavya@rowy.io.rowy",
      "timestamp": { "_seconds": 1713772268, "_nanoseconds": 360000000 }
    },
    "syncIndex": {
      "ranBy": "bhavya@rowy.io.rowy",
      "status": "synced",
      "completedAt": { "_seconds": 1713772268, "_nanoseconds": 392000000 }
    },
    "integrity": "v3:bef960ae8852752756e1b34c34e8a3b5",
    "src": "https://storage.googleapis.com/buildship-app-us-central1/publicLib/nodes/@buildship/groq-chat/1.1.0/build.cjs",
    "id": "groq-chat",
    "_libRef": {
      "integrity": "v3:5ce66c9846c16259a1726ff78a6e5005",
      "libNodeRefId": "@buildship/groq-chat",
      "isDirty": false,
      "libType": "public",
      "src": "https://storage.googleapis.com/buildship-app-us-central1/publicLib/nodes/@buildship/groq-chat/1.0.2/build.cjs",
      "version": "1.0.2"
    },
    "version": "1.1.0",
    "script": "import Groq from 'groq-sdk';\n\nexport default async ({ apiKey, systemPrompt, userPrompt, model, temp, isJSON }: NodeInputs,{logging}: NodeScriptOptions) : NodeOutput  => {\nconst groq = new Groq({\n    apiKey: apiKey\n});\n  const completion = await groq.chat.completions.create({\n      messages: [\n            {\n                role: \"system\",\n                content: systemPrompt\n            },\n            {\n                role: \"user\",\n                content: userPrompt\n            }\n        ],\n      model: model,\n      temperature: temp\n  });\n  return completion.choices[0]?.message?.content || \"No Response\"\n}",
    "failCount": 69,
    "deployedAt": { "_seconds": 1717467670, "_nanoseconds": 52000000 },
    "usageCounter": 538
  }
]
